{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"noValueCode.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1BaP74_saIOofRAl9rY8Okpj_8owMVTs9","authorship_tag":"ABX9TyP2Js4ic/vfzhJzOsxcPUVc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4jCf6LRR5qI8","executionInfo":{"status":"ok","timestamp":1650320443423,"user_tz":240,"elapsed":60065,"user":{"displayName":"Hu Chunjun","userId":"03301405306660133263"}},"outputId":"f554cc27-8891-42af-edf5-489007d0b81d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Successfully uninstalled tensorflow-2.8.0\n","Collecting tensorflow-gpu==1.14.0\n","  Downloading tensorflow_gpu-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (377.1 MB)\n","\u001b[K     |████████████████████████████████| 377.1 MB 8.8 kB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.1.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.2.0)\n","Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n","  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n","\u001b[K     |████████████████████████████████| 488 kB 91.6 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.44.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.15.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.21.5)\n","Collecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (3.17.3)\n","Collecting tensorboard<1.15.0,>=1.14.0\n","  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 68.3 MB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.5.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.37.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.0.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (3.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.3.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (57.4.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.8.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (1.5.2)\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow-gpu\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n","Collecting keras==2.2.5\n","  Downloading Keras-2.2.5-py2.py3-none-any.whl (336 kB)\n","\u001b[K     |████████████████████████████████| 336 kB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.1.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.0.8)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.21.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.5) (1.5.2)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","Successfully installed keras-2.2.5\n","Collecting h5py==2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 8.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.21.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n","Installing collected packages: h5py\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","Successfully installed h5py-2.10.0\n"]}],"source":["!pip uninstall -y tensorflow\n","!pip install tensorflow-gpu==1.14.0\n","!pip install keras==2.2.5\n","!pip install 'h5py==2.10.0'"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/MyDrive/daydayup/Morpho')\t# 设置工作路径"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5OyL6i67ZVF","executionInfo":{"status":"ok","timestamp":1650329802267,"user_tz":240,"elapsed":840,"user":{"displayName":"Hu Chunjun","userId":"03301405306660133263"}},"outputId":"c380eca8-9e82-4b77-953f-d035450aa50e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ruWlfu77hle","executionInfo":{"status":"ok","timestamp":1650322739897,"user_tz":240,"elapsed":151,"user":{"displayName":"Hu Chunjun","userId":"03301405306660133263"}},"outputId":"0e887753-63e1-4de6-fc22-c37b4eb7a10f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 12394369102423128752\n",", name: \"/device:XLA_CPU:0\"\n","device_type: \"XLA_CPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 13236313232956875485\n","physical_device_desc: \"device: XLA_CPU device\"\n",", name: \"/device:XLA_GPU:0\"\n","device_type: \"XLA_GPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 11171241377108803926\n","physical_device_desc: \"device: XLA_GPU device\"\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 15964005991\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 16451096445254548253\n","physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n","]\n"]}]},{"cell_type":"code","source":["import keras\n","import os\n","from keras import layers,models,optimizers\n","from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n","from keras.layers import *   \n","from keras.models import Sequential, Model\n","import keras.backend as K\n","from keras import regularizers\n","from keras.applications.xception import Xception,preprocess_input\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.inception_v3 import InceptionV3\n","import matplotlib.pylab as plt\n","from keras.optimizers import Adam\n","from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","import random\n","import sys\n","import numpy as np\n","from keras import backend as K\n","from keras.models import load_model\n","from keras.optimizers import SGD\n","import tensorflow as tf\n","from tensorflow import keras as ke\n","from keras.preprocessing.image import img_to_array, ImageDataGenerator#图片转为array\n","from keras.utils import to_categorical#相当于one-hot\n","from imutils import paths\n","import cv2\n","import numpy as np\n","import random\n","import os\n","from keras.callbacks import TensorBoard, ModelCheckpoint\n","import numpy as np\n","\n","from keras.layers import Conv2D, Dense, LeakyReLU, Dropout, Input\n","from keras.layers import Reshape, Conv2DTranspose, Flatten\n","from keras.layers import BatchNormalization\n","from keras.models import Model\n","from keras import optimizers\n","import keras\n","from decimal import Decimal, getcontext\n","from PIL import Image\n","from tensorflow.contrib.data import prefetch_to_device, shuffle_and_repeat, map_and_batch\n","from tensorflow.contrib.opt import MovingAverageOptimizer\n","import argparse\n","import tensorflow as tf\n","import scipy.misc\n","import numpy as np\n","import os\n","from glob import glob\n","\n","import tensorflow as tf\n","import tensorflow.contrib.slim as slim\n","from keras.datasets import cifar10, mnist\n","\n","\n","\n","import os\n","from PIL import Image\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"],"metadata":{"id":"SXEc3wWo7Vs9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 使用Gan训练cifra10，测试使用，由于BigGan遇到了处理不了的问题，回来试一下Gan"],"metadata":{"id":"D1OMFgjkIiS7"}},{"cell_type":"code","source":["# conf\n","latent_dim = 28\n","height = 28\n","width = 28\n","channels = 1\n","\n","generator_input = keras.Input(shape=(latent_dim,))\n","\n","# Generate part\n","x = layers.Dense(128 * 14 * 14)(generator_input)\n","x = layers.LeakyReLU()(x)\n","x = layers.Reshape((14, 14, 128))(x)\n","x = layers.Conv2D(256, 5, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(256, 5, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(256, 5, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","# use tanh as the activation function in Generator part\n","x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n","generator = keras.models.Model(generator_input, x)\n","generator.summary()"],"metadata":{"id":"vxWjpn4-IrRE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["discriminator_input = layers.Input(shape=(height, width, channels))\n","x = layers.Conv2D(128, 3)(discriminator_input)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(128, 4, strides=2)(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(128, 4, strides=2)(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(128, 4, strides=2)(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Flatten()(x)\n","\n","# Dropout for preventing Discriminator part controlling the whole network!!!!  \n","x = layers.Dropout(0.8)(x)\n","\n","# Classification\n","x = layers.Dense(1, activation='sigmoid')(x)\n","\n","discriminator = keras.models.Model(discriminator_input, x)\n","discriminator.summary()\n","\n","# set optimizer -> learning rate decay and gradient clipping\n","discriminator_optimizer = keras.optimizers.RMSprop(lr=0.00001, clipvalue=1.0, decay=1e-8)\n","discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"],"metadata":{"id":"kRvpei0-JGlR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set weights of Discriminator part to non-trainable for Generator part\n","discriminator.trainable = False\n","\n","gan_input = keras.Input(shape=(latent_dim,))\n","gan_output = discriminator(generator(gan_input))\n","gan = keras.models.Model(gan_input, gan_output)\n","\n","gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n","gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"],"metadata":{"id":"2dZwCO79JGpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set steps and parameters\n","iterations = 4500\n","batch_size = 20\n","\n","# start training\n","start = 0\n","for step in range(iterations):\n","    \n","    # train Discriminator part\n","    # initial input data/latent vectors\n","    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n","    # generate faked pic\n","    generated_images = generator.predict(random_latent_vectors)\n","    # mix faked images with real images\n","    stop = start + batch_size\n","    real_images = train_data[start: stop]\n","    combined_images = np.concatenate([generated_images, real_images])\n","    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n","    # mix noise\n","    labels += 0.05 * np.random.random(labels.shape)\n","    # train the Discriminator part\n","    d_loss = discriminator.train_on_batch(combined_images, labels)\n","\n","    # train GAN part\n","    # initial input data/latent vectors\n","    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n","    # set labels for faked images\n","    misleading_targets = np.zeros((batch_size, 1))\n","    # train GAN part and freeze discriminator\n","    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n","\n","    # judge when to stop\n","    start += batch_size\n","    if start > len(train_data) - batch_size:\n","    start = 0\n","\n","    # output log\n","    if step % 100 == 0:\n","        print('discriminator loss at step %s: %s' % (step, d_loss))\n","        print('adversarial loss at step %s: %s' % (step, a_loss))"],"metadata":{"id":"a5lLatrnJGsv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate test data(#40)\n","random_latent_vectors = np.random.normal(size=(40, latent_dim))\n","# decode input data to faked images\n","generated_images = generator.predict(random_latent_vectors)\n","\n","# print the shape of faked images\n","print(np.array(generated_images[2]).shape)\n","\n","# plot\n","for i in range(generated_images.shape[0]):\n","    img = image.array_to_img(generated_images[i] * 255., scale=False)\n","    plt.figure()\n","    plt.imshow(img,cmap='gray')\n","    \n","plt.show()"],"metadata":{"id":"0HyJbEOAKQgo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 关于sneaker数据集的各种功能函数，基于keras，绝大部分作废"],"metadata":{"id":"ft9gsFqZIWhN"}},{"cell_type":"code","source":["seed = 42\n","\n","os.environ['PYTHONHASHSEED']=str(seed)\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n","os.environ['HOROVOD_FUSION_THRESHOLD']='0'\n","random.seed(seed)\n","np.random.seed(seed)\n","tf.random.set_random_seed(seed)\n","tf.set_random_seed(seed)"],"metadata":{"id":"yLuxZH0B7urv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# conf\n","channel = 3\n","height = 128 #300\n","width = 128 #400\n","class_num = 2 # 4\n","#norm_size = 32#参数\n","batch_size = 64\n","epochs = 200\n","\n","\n","train_dir = './drive/MyDrive/daydayup/dataset/sneaker_nonsneaker/sneaker_nonsneaker/training' # ../data/dataset/train\n","validation_dir = './drive/MyDrive/daydayup/dataset/sneaker_nonsneaker/sneaker_nonsneaker/testing' # ../data/dataset/val\n","# train_dir = './drive/MyDrive/daydayup/dataset/filteredDataset/training'\n","# validation_dir = './drive/MyDrive/daydayup/dataset/filteredDataset/testing'\n","save_tl_dir = \"./drive/MyDrive/daydayup/Morpho/predict/TLCheckpoint\"\n","save_ft_dir = \"./drive/MyDrive/daydayup/Morpho/predict/FTCheckpoint\"\n","save_Direct_dir = \"./drive/MyDrive/daydayup/Morpho/predict/DirectCheckpoint\"\n","\n","\n","totalTrain = len(list(paths.list_images(train_dir)))\n","totalVal = len(list(paths.list_images(validation_dir)))\n","print(totalTrain)\n","print(totalVal)"],"metadata":{"id":"GyNhoCjr7uwV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650234658791,"user_tz":240,"elapsed":7831,"user":{"displayName":"Hu Chunjun","userId":"03301405306660133263"}},"outputId":"c8a79bb8-4dac-43d6-821a-74d5781b6eef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1787\n","440\n"]}]},{"cell_type":"code","source":["source_train_dir_positive = os.path.join(train_dir, 'positive')\n","source_train_dir_negative = os.path.join(train_dir, 'negative')\n","source_validation_dir_positive = os.path.join(validation_dir, 'positive')\n","source_validation_dir_negative = os.path.join(validation_dir, 'negative')"],"metadata":{"id":"6qpHahmUe3gV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"],"metadata":{"id":"pN14i-cce0I6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","# train_dir_positive = os.path.join(train_dir, '/1')\n","# train_dir_negative = os.path.join(train_dir, '/0')\n","# validation_dir_positive = os.path.join(validation_dir, '/1')\n","# validation_dir_negative = os.path.join(validation_dir, '/0')\n","\n","def pilConvertJPG(path):\n","    for a, _, c in os.walk(path):\n","        for n in c:\n","          # print(n)\n","          if '.jpg' in n or '.png' in n or '.jpeg' in n or '.JPEG' in n:\n","              img = Image.open(os.path.join(a, n))\n","              rgb_im = img.convert('RGB')\n","              error_img_path = os.path.join(a,n)\n","              os.remove(error_img_path)\n","              n = ''.join(filter(lambda n: ord(n) < 256, n))\n","              jpg_img_path = os.path.splitext(os.path.join(a, n).replace('\\\\', '/'))[0]\n","              jpg_img_path += '.jpg'\n","              # print(jpg_img_path)\n","              rgb_im.save(jpg_img_path)\n","          else:\n","            print(\"error:\", n)\n","\n","\n"],"metadata":{"id":"LpHZ9UgkFdEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pilConvertJPG(\"./drive/MyDrive/daydayup/Morpho/BigGan/dataset/groundTruth\")"],"metadata":{"id":"BPZnlz13e5nE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pilConvertJPG(source_train_dir_positive)\n","pilConvertJPG(source_train_dir_negative)\n","pilConvertJPG(source_validation_dir_positive)\n","pilConvertJPG(source_validation_dir_negative)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3niodnVFdGc","executionInfo":{"status":"ok","timestamp":1650176446368,"user_tz":240,"elapsed":111259,"user":{"displayName":"Hu Chunjun","userId":"03301405306660133263"}},"outputId":"9d643eac-b507-4505-86d2-eb60bd158b08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"]}]},{"cell_type":"code","source":["def dataprocess(train_dir, validation_dir,height, width, batch_size):\n","    train_datagen = ImageDataGenerator(\n","        rescale=1./255,\n","        rotation_range=40,\n","        width_shift_range=0.2,\n","        height_shift_range=0.2,\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        fill_mode='nearest')\n","    test_datagen = ImageDataGenerator(rescale=1./255)\n","    train_generator = train_datagen.flow_from_directory(\n","        train_dir,\n","        target_size=(height, width),\n","        batch_size= batch_size,\n","        class_mode='categorical')\n","    validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(height, width),\n","        batch_size= batch_size,\n","        class_mode='categorical')\n","    return train_generator, validation_generator\n","\n","\n"],"metadata":{"id":"T_WYX1zHt8RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_generator, validation_generator = dataprocess(train_dir, validation_dir, height, width, batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYenaD2vAed5","executionInfo":{"status":"ok","timestamp":1650240682202,"user_tz":240,"elapsed":300,"user":{"displayName":"Hu Chunjun","userId":"03301405306660133263"}},"outputId":"2d8caeb0-21e2-435c-c524-615336a822de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1787 images belonging to 2 classes.\n","Found 440 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["# generator使用方法，先利用生成器生成数据然后训练\n","for i, j in train_generator:\n","  print(i.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":710},"id":"vLkNOJcjbk9Q","executionInfo":{"status":"error","timestamp":1650248754165,"user_tz":240,"elapsed":51405,"user":{"displayName":"Hu Chunjun","userId":"03301405306660133263"}},"outputId":"48475cf5-aebb-4803-e23b-c6be92b36999"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 112, 112, 3)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-52935e0de519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                            interpolation=self.interpolation)\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# 编写网络\n","\n","class Generator:\n","  def neural(latent_dim):\n","    input_shape = (latent_dim,)\n","\n","    inputs = Input(shape= input_shape)\n","\n","    \n","\n","    # conv_base = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n","    # x = conv_base.output\n","\n","    # UpSample = MaxPooling2D(pool_size=(9, 9), strides=(1, 1),padding = 'same', name='MaxPooling2D')(inputs)\n","    # UpSample = Dropout(0.5)(UpSample)\n","    # UpSample = Conv2D(256,(1,1))(UpSample)\n","    # UpSample = BatchNormalization()(UpSample)\n","    # UpSample = Activation('relu')(UpSample)\n","    # UpSample = Dropout(0.5)(UpSample)\n","    # UpSample = Conv2D(64,(1,1))(UpSample)\n","    # UpSample = BatchNormalization()(UpSample)\n","    # UpSample = Activation('relu')(UpSample)\n","    # UpSample = Dropout(0.5)(UpSample)\n","    # UpSample = Flatten(name='flatten')(UpSample)\n","    # UpSample = Dense(classes)(UpSample)\n","    # UpSample = BatchNormalization()(UpSample)\n","    # predictions = Activation('softmax')(UpSample)\n","\n","    model = Model(inputs=inputs, outputs=predictions)\n","    \n","\n","    return model\n","\n","generator_input = keras.Input(shape=(latent_dim,))\n","\n","# Generate part\n","x = layers.Dense(128 * 14 * 14)(generator_input)\n","x = layers.LeakyReLU()(x)\n","x = layers.Reshape((14, 14, 128))(x)\n","x = layers.Conv2D(256, 5, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(256, 5, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","x = layers.Conv2D(256, 5, padding='same')(x)\n","x = layers.LeakyReLU()(x)\n","# use tanh as the activation function in Generator part\n","x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n","generator = keras.models.Model(generator_input, x)\n","generator.summary()"],"metadata":{"id":"kaKEXhYLbk_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OLUWhJGvblCD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 废案,想自己写generate函数来生成图片，但是遇见了无法处理的问题，主要还是读取那一块的问题，tf自带的读取方法没有理解，不过好在keras的读取方式也可以用\n","\n","\n","# for i in os.listdir(train_dir):\n","#   print(i)\n","\n","\n","# 这个函数用于返回符合,可以使用正则路径，*表示任意字符\n","# path_list = tf.data.Dataset.list_files(train_path + \"*.jpg\")\n","\n","\n","\n","\n","# 定义一个读取图片的函数\n","def read_image(dirPath, batchSize, k, classNum = 2):\n","    '''\n","    :dirPath: 数据集读取路径\n","    :batchSize: 获得的数据数量\n","    :k: 记录历史获取次数\n","    :yield: 该步图片张量列表与图片标签列表\n","    '''\n","    historyCheck = np.zeros(classNum)\n","    ratioNum = []\n","    splitRatio = np.random.dirichlet(np.ones(classNum),size=1)\n","    for i in range(classNum):\n","      ratioNum.append(int(splitRatio[0][i] * batchSize))\n","\n","    for className in os.listdir(dirPath):\n","      content = os.path.join(dirPath, str(className))\n","      data = []  # 图片聊表\n","      labels = []  # 图片标签列表\n","\n","      path_list = tf.data.Dataset.list_files(content + \"*.jpg\")\n","\n","      # 根据文件路径列表依次读取\n","      for i in path_list:\n","          image_temp = tf.io.read_file(i)  # tesnsorflow的io读取文件\n","          image_temp = tf.image.decode_jpeg(image_temp)  # 根据图片的格式进行编码转化为张量，这里图片是jpg格式\n","          data.append(image_temp)  # 图片加入到数据集\n","          labels.append(str(className))  # 获取文件名加入到标签，这里要张量i转化为字符串\n","\n","    \n","\n","    for index, item in enumerate(ratioNum):\n","      historyCheck[index] = historyCheck[index] + item\n","    yield np.array(data), np.array(labels)\n","\n","\n","# 读取训练图片\n","train_images, train_labels = read_image(train_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhZHKwFSR_Jh","outputId":"0b3bf45b-f4ed-46ca-a1d8-2424b71d34f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"]}]}]}